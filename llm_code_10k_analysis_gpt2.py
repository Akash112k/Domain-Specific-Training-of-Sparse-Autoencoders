# -*- coding: utf-8 -*-
"""LLM_code_10k_analysis_gpt2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tPEgM0ayUfkxZwUlY-17KPBRsYOFNqzH
"""

!pip install sae-lens

import torch
import os

from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner

if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"

print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

!huggingface-cli login

from huggingface_hub import login
login(token="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")

total_training_steps = 30_000  # probably we should do more
batch_size = 4096
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training
l1_warm_up_steps = total_training_steps // 20  # 5% of training

cfg = LanguageModelSAERunnerConfig(
    # Data Generating Function (Model + Training Distibuion)
    model_name="gpt2-small",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)
    hook_name="blocks.0.hook_mlp_out",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)
    hook_layer=0,  # Only one layer in the model.
    d_in=768,  # the width of the mlp output.
    dataset_path="NeelNanda/code-10k",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.
    is_dataset_tokenized=False,
    streaming=True,  # we could pre-download the token dataset if it was small.
    # SAE Parameters
    mse_loss_normalization=None,  # We won't normalize the mse loss,
    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.
    b_dec_init_method="zeros",  # The geometric median can be used to initialize the decoder weights.
    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.
    normalize_sae_decoder=False,
    scale_sparsity_penalty_by_decoder_norm=True,
    decoder_heuristic_init=True,
    init_encoder_as_decoder_transpose=True,
    normalize_activations="expected_average_only_in",
    # Training Parameters
    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.
    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)
    adam_beta2=0.999,
    lr_scheduler_name="constant",  # constant learning rate with warmup. Could be better schedules out there.
    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.
    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.
    l1_coefficient=5,  # will control how sparse the feature activations are
    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.
    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)
    train_batch_size_tokens=batch_size,
    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.
    # Activation Store Parameters
    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.
    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.
    store_batch_size_prompts=16,
    # Resampling protocol
    use_ghost_grads=False,  # we don't use ghost grads anymore.
    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats
    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.
    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.
    # WANDB
    log_to_wandb=True,  # always use wandb unless you are just testing code.
    wandb_project="CODE10K",
    wandb_log_frequency=30,
    eval_every_n_wandb_logs=20,
    # Misc
    device=device,
    seed=42,
    n_checkpoints=0,
    checkpoint_path="checkpoints",
    dtype="float32",
)
# look at the next cell to see some instruction for what to do while this is running.
sparse_autoencoder = SAETrainingRunner(cfg).run()



sparse_autoencoder

import numpy as np
import torch
import plotly_express as px

from transformer_lens import HookedTransformer

# Model Loading
from sae_lens import SAE
from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list

# Virtual Weight / Feature Statistics Functions
from sae_lens.analysis.feature_statistics import (
    get_all_stats_dfs,
    get_W_U_W_dec_stats_df,
)

# Enrichment Analysis Functions
from sae_lens.analysis.tsea import (
    get_enrichment_df,
    manhattan_plot_enrichment_scores,
    plot_top_k_feature_projections_by_token_and_category,
)
from sae_lens.analysis.tsea import (
    get_baby_name_sets,
    get_letter_gene_sets,
    generate_pos_sets,
    get_test_gene_sets,
    get_gene_set_from_regex,
)

# +# Assume `sae` is your trained Sparse Autoencoder instance
save_path = "sae_saved"  # Directory where the model will be saved

# Save the model using the `save_model` method of the instance
SAE.save_model(sparse_autoencoder,save_path)

print(f"SAE model successfully saved at {save_path}.")

# Load the model
load_path = "sae_saved"
device = "cpu"  # or "cuda" if you want to load it on GPU
dtype = "float32"  # Optional: Specify the dtype if needed

# Load the Sparse Autoencoder
loaded_sae = SAE.load_from_pretrained(load_path, device=device, dtype=dtype)
print("SAE successfully loaded.")

sparse_autoencoder

pip install sae-dashboard

from sae_lens import SAE
from transformer_lens import HookedTransformer
from sae_dashboard.sae_vis_data import SaeVisConfig
from sae_dashboard.sae_vis_runner import SaeVisRunner

# Load model and SAE
model = HookedTransformer.from_pretrained("gpt2-small", device="cuda", dtype="bfloat16")

# sae, _, _ = SAE.from_pretrained(
#     release="gpt2-small-res-jb",
#     sae_id="blocks.6.hook_resid_pre",
#     device="cuda"
# )
loaded_sae.fold_W_dec_norm()

sparse_autoencoder.fold_W_dec_norm()

from safetensors.torch import load_file

# Load all tensors as a dictionary
file_path = "/content/checkpoints/ttx7tor8/final_40960000/sparsity.safetensors"
tensors = load_file(file_path)

# Print all tensors
for name, tensor in tensors.items():
    print(f"Tensor Name: {name}")
    print(f"Tensor Shape: {tensor.shape}")
    print(f"Tensor: {tensor}")

load_path

model = HookedTransformer.from_pretrained("gpt2-small")
# this is an outdated way to load the SAE. We need to have feature spartisity loadable through the new interface to remove it.
gpt2_small_sparse_autoencoders = {}
gpt2_small_sae_sparsities = {}

for layer in range(12):
    sae, original_cfg_dict, sparsity = SAE.load_from_pretrained(load_path), cfg,tensors
    gpt2_small_sparse_autoencoders[f"blocks.{layer}.hook_resid_pre"] = sparse_autoencoder
    gpt2_small_sae_sparsities[f"blocks.{layer}.hook_resid_pre"] = sparsity

# In the post, I focus on layer 8
layer = 8

# get the corresponding SAE and feature sparsities.
sparse_autoencoder = gpt2_small_sparse_autoencoders[f"blocks.{layer}.hook_resid_pre"]
log_feature_sparsity = gpt2_small_sae_sparsities[f"blocks.{layer}.hook_resid_pre"]

W_dec = sparse_autoencoder.W_dec.detach().cpu()

# calculate the statistics of the logit weight distributions
W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(
    W_dec, model, cosine_sim=False
)
W_U_stats_df_dec["sparsity"] = (
    log_feature_sparsity  # add feature sparsity since it is often interesting.
)
display(W_U_stats_df_dec)

# Let's look at the distribution of the 3rd / 4th moments. I found these aren't as useful on their own as joint distributions can be.
px.histogram(
    W_U_stats_df_dec,
    x="skewness",
    width=800,
    height=300,
    nbins=1000,
    title="Skewness of the Logit Weight Distributions",
).show()

px.histogram(
    W_U_stats_df_dec,
    x=np.log10(W_U_stats_df_dec["kurtosis"]),
    width=800,
    height=300,
    nbins=1000,
    title="Kurtosis of the Logit Weight Distributions",
).show()

fig = px.scatter(
    W_U_stats_df_dec,
    x="skewness",
    y="kurtosis",
    color="std",
    color_continuous_scale="Portland",
    hover_name="feature",
    width=800,
    height=500,
    log_y=True,  # Kurtosis has larger outliers so logging creates a nicer scale.
    labels={"x": "Skewness", "y": "Kurtosis", "color": "Standard Deviation"},
    title=f"Layer {8}: Skewness vs Kurtosis of the Logit Weight Distributions",
)

# decrease point size
fig.update_traces(marker=dict(size=3))


fig.show()

nltk.download()

import nltk

nltk.download("averaged_perceptron_tagger")
# get the vocab we need to filter to formulate token sets.
vocab = model.tokenizer.get_vocab()  # type: ignore

# make a regex dictionary to specify more sets.
regex_dict = {
    # Keywords for Python
    "python_keywords": r"^(def|return|if|else|elif|for|while|break|continue|import|from|as|try|except|finally|with|lambda|class|pass|raise|yield|assert|del|global|nonlocal|True|False|None|and|or|not|is|in)$",

    # Common operators
    "operators": r"^(\+|\-|\*|\/|%|\*\*|\/\/|==|!=|<=|>=|<|>|\=|\+=|\-=|\*=|\/=|%=|&|\||\^|~|<<|>>)$",

    # Delimiters and punctuation
    "delimiters": r"^(\(|\)|\[|\]|\{|\}|,|:|;|\.|@|=|->|=>|\.\.\.)$",

    # Numeric literals
    "numeric_literals": r"^\d+(\.\d+)?$",

    # String literals (simple heuristic)
    "string_literals": r"^(['\"])(.*?)(\1)$",

    # Identifiers (variable/function names)
    "identifiers": r"^[a-zA-Z_]\w*$",

    # Comments (assuming single-line comments start with '#')
    "comments": r"^#.*$",

    # Decorators in Python
    "decorators": r"^@.*$",

    # Function calls (basic heuristic for tokens with parentheses at the end)
    "function_calls": r"^[a-zA-Z_]\w*\(.*\)$",

    # Module names (assuming lowercase with optional underscores, e.g., 'os', 'numpy')
    "module_names": r"^[a-z]+(_[a-z]+)*$",

    # Class names (PascalCase, e.g., 'MyClass', 'TestCase')
    "class_names": r"^[A-Z][a-zA-Z0-9]*$",

    # Method names (snake_case, typically prefixed with 'self.')
    "method_names": r"^self\.[a-z]+(_[a-z]+)*$",

    # Constants (all uppercase with underscores, e.g., 'MAX_VALUE', 'PI')
    "constants": r"^[A-Z]+(_[A-Z]+)*$",

    # List comprehensions (tokens containing brackets and 'for', e.g., '[x for x in y]')
    "list_comprehensions": r"^\[.* for .* in .*\]$",

    # Dict comprehensions (tokens containing braces and 'for', e.g., '{k: v for k, v in pairs}')
    "dict_comprehensions": r"^\{.*:.* for .* in .*\}$",

    # F-strings (Python 3.6+ formatted strings, e.g., 'f"Hello {name}"')
    "f_strings": r"^f['\"].*?\{.*?\}.*['\"]$",

    # Import statements (heuristic for tokens starting with 'import' or 'from')
    "import_statements": r"^(import|from) [a-zA-Z_][a-zA-Z0-9_\.]*$",

    # Special dunder methods (__init__, __str__, etc.)
    "dunder_methods": r"^__[a-zA-Z_]+__$",

    # Keywords for asynchronous programming (async/await constructs)
    "async_keywords": r"^(async|await)$",
}


# print size of gene sets
all_token_sets = get_letter_gene_sets(vocab)
for key, value in regex_dict.items():
    gene_set = get_gene_set_from_regex(vocab, value)
    all_token_sets[key] = gene_set

# some other sets that can be interesting
baby_name_sets = get_baby_name_sets(vocab)
pos_sets = generate_pos_sets(vocab)
arbitrary_sets = get_test_gene_sets(model)

all_token_sets = {**all_token_sets, **pos_sets}
all_token_sets = {**all_token_sets, **arbitrary_sets}
all_token_sets = {**all_token_sets, **baby_name_sets}

# for each gene set, convert to string and  print the first 5 tokens
for token_set_name, gene_set in sorted(
    all_token_sets.items(), key=lambda x: len(x[1]), reverse=True
):
    tokens = [model.to_string(id) for id in list(gene_set)][:10]  # type: ignore
    print(f"{token_set_name}, has {len(gene_set)} genes")
    print(tokens)
    print("----")

# Display all regex token sets with their sizes and samples
all_token_sets = {}

# Apply regex patterns to the vocabulary
for name, pattern in regex_dict.items():
    gene_set = get_gene_set_from_regex(vocab, pattern)  # Apply regex to filter tokens
    all_token_sets[name] = gene_set

# Display the size and a sample of tokens for each token set
for token_set_name, tokens in sorted(all_token_sets.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"Token Set: {token_set_name}")
    print(f"Size: {len(tokens)} tokens")
    print(f"Sample: {list(tokens)[:10]}")  # Show the first 10 tokens
    print("----")

features_ordered_by_skew = (
    W_U_stats_df_dec["skewness"].sort_values(ascending=False).head(5000).index.to_list()
)

# Filter our list with code-specific token sets.
token_sets_index = [
    "python_keywords",
    "operators",
    "numeric_literals",
    "string_literals",
    "camelCase",
    "snake_case",
    "PascalCase",
    "comments",
]

# Select token sets based on the updated token_sets_index
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}

# Calculate the enrichment scores
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U,  # use the logit weight values as our rankings over tokens.
    features_ordered_by_skew,  # subset by these features
    token_set_selected,  # use token_sets
)

# Plot the enrichment scores
manhattan_plot_enrichment_scores(
    df_enrichment_scores, label_threshold=0, top_n=3  # use our enrichment scores
).show()

fig = px.scatter(
    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T,
    x="python_keywords",
    y="operators",
    marginal_x="histogram",
    marginal_y="histogram",
    labels={
        "python_keywords": "Python Keywords",
        "operators": "Operators",
    },
    title="Enrichment Scores for Python Keywords vs Operators",
    height=800,
    width=800,
)
# Reduce point size on the scatter only
fig.update_traces(marker=dict(size=2), selector=dict(mode="markers"))
fig.show()

# Update token sets index with relevant programming-related sets
token_sets_index = ["numeric_literals", "operators", "camelCase", "PascalCase"]

# Select the token sets based on the updated index
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}

# Calculate the enrichment scores
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U,  # Logit weight values as rankings over tokens
    features_ordered_by_skew,  # Subset by skew-ordered features
    token_set_selected,  # Use the selected token sets
)

# Plot the enrichment scores
manhattan_plot_enrichment_scores(
    df_enrichment_scores,
    label_threshold=0.1,  # Highlight tokens with significant scores
    top_n=5,  # Display the top 5 enriched token sets
).show()

# Specify the hook point you're using, and the features you're analyzing
sae_vis_config = SaeVisConfig(
    hook_point = utils.get_act_name("post", 0),
    features = range(64),
    verbose = True,
)

# Gather the feature data
sae_vis_data = SaeVisData.create(
    encoder = encoder,
    encoder_B = encoder_B,
    model = model,
    tokens = all_tokens[:2048],
    cfg = sae_vis_config,
)

# Save as HTML file & display vis
filename = "_feature_vis_demo.html"
sae_vis_data.save_feature_centric_vis(filename, feature_idx=8)

display_vis_inline(filename)

# Update token sets index with relevant programming-related sets
token_sets_index = ["python_keywords", "string_literals", "comments"]

# Select the token sets based on the updated index
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}

# Calculate the enrichment scores
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U,  # Logit weight values as rankings over tokens
    features_ordered_by_skew,  # Subset by skew-ordered features
    token_set_selected,  # Use the selected token sets
)

# Plot the enrichment scores
manhattan_plot_enrichment_scores(
    df_enrichment_scores,
    label_threshold=0.05,  # Highlight tokens with significant scores
    top_n=3,  # Display the top 3 enriched token sets
).show()

# Update token sets index with relevant programming-related sets
token_sets_index = ["module_names", "identifiers"]

# Select the token sets based on the updated index
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}

# Calculate the enrichment scores
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U,  # Logit weight values as rankings over tokens
    features_ordered_by_skew,  # Subset by skew-ordered features
    token_set_selected,  # Use the selected token sets
)

# Plot the enrichment scores
manhattan_plot_enrichment_scores(
    df_enrichment_scores,
    label_threshold=0.1,  # Highlight tokens with significant scores
    top_n=3,  # Display the top 3 enriched token sets
).show()

# Transform enrichment scores
tmp_df = df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T

# Define color based on skewness from W_U_stats_df_dec
color = (
    W_U_stats_df_dec.sort_values("skewness", ascending=False)
    .head(5000)["skewness"]
    .values
)

# Create a scatter plot for module_names and identifiers
fig = px.scatter(
    tmp_df.reset_index().rename(columns={"index": "feature"}),
    x="module_names",
    y="identifiers",
    marginal_x="histogram",
    marginal_y="histogram",
    # color=color,  # Uncomment if you want to color by skewness
    labels={
        "module_names": "Enrichment Score (Module Names)",
        "identifiers": "Enrichment Score (Identifiers)",
    },
    height=600,
    width=800,
    hover_name="feature",
)

# Reduce point size on the scatter only
fig.update_traces(marker=dict(size=3), selector=dict(mode="markers"))

# Annotate features where the absolute distance between module_names and identifiers > 2.9
for feature in df_enrichment_scores.columns:
    if abs(tmp_df["module_names"][feature] - tmp_df["identifiers"][feature]) > 2.9:
        fig.add_annotation(
            x=tmp_df["module_names"][feature] - 0.4,
            y=tmp_df["identifiers"][feature] + 0.1,
            text=f"{feature}",
            showarrow=False,
        )

fig.show()

for category in ["module_names"]:
    plot_top_k_feature_projections_by_token_and_category(
        token_set_selected,
        df_enrichment_scores,
        category=category,
        dec_projection_onto_W_U=dec_projection_onto_W_U,
        model=model,
        log_y=False,
        histnorm=None,
    )

token_sets_index = ["nltk_pos_WP", "nltk_pos_RBR", "nltk_pos_WDT", "nltk_pos_RB"]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores).show()



W_U_stats_df_dec_all_layers = get_all_stats_dfs(
    gpt2_small_sparse_autoencoders, gpt2_small_sae_sparsities, model, cosine_sim=True
)

display(W_U_stats_df_dec_all_layers.shape)
display(W_U_stats_df_dec_all_layers.head())

token_sets_index = ["negative_words", "positive_words"]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores).show()

fig = px.scatter(
    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x))
    .T.reset_index()
    .rename(columns={"index": "feature"}),
    x="negative_words",
    y="positive_words",
    marginal_x="histogram",
    marginal_y="histogram",
    labels={
        "starts_with_space": "Starts with Space",
        "starts_with_capital": "Starts with Capital",
    },
    title="Enrichment Scores for Starts with Space vs Starts with Capital",
    height=800,
    width=800,
    hover_name="feature",
)
# reduce point size on the scatter only
fig.update_traces(marker=dict(size=2), selector=dict(mode="markers"))
fig.show()

token_sets_index = ["contains_close_bracket", "contains_open_bracket"]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores).show()

token_sets_index = [
    "1910's",
    "1920's",
    "1930's",
    "1940's",
    "1950's",
    "1960's",
    "1970's",
    "1980's",
    "1990's",
    "2000's",
    "2010's",
]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores).show()

token_sets_index = ["positive_words", "negative_words"]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores, label_threshold=0.98).show()

token_sets_index = ["boys_names", "girls_names"]
token_set_selected = {
    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index
}
df_enrichment_scores = get_enrichment_df(
    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected
)
manhattan_plot_enrichment_scores(df_enrichment_scores).show()

tmp_df = df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T
color = (
    W_U_stats_df_dec.sort_values("skewness", ascending=False)
    .head(5000)["skewness"]
    .values
)
fig = px.scatter(
    tmp_df.reset_index().rename(columns={"index": "feature"}),
    x="boys_names",
    y="girls_names",
    marginal_x="histogram",
    marginal_y="histogram",
    # color = color,
    labels={
        "boys_names": "Enrichment Score (Boys Names)",
        "girls_names": "Enrichment Score (Girls Names)",
    },
    height=600,
    width=800,
    hover_name="feature",
)
# reduce point size on the scatter only
fig.update_traces(marker=dict(size=3), selector=dict(mode="markers"))
# annotate any features where the absolute distance between boys names and girls names > 3
for feature in df_enrichment_scores.columns:
    if abs(tmp_df["boys_names"][feature] - tmp_df["girls_names"][feature]) > 2.9:
        fig.add_annotation(
            x=tmp_df["boys_names"][feature] - 0.4,
            y=tmp_df["girls_names"][feature] + 0.1,
            text=f"{feature}",
            showarrow=False,
        )


fig.show()

for category in ["boys_names"]:
    plot_top_k_feature_projections_by_token_and_category(
        token_set_selected,
        df_enrichment_scores,
        category=category,
        dec_projection_onto_W_U=dec_projection_onto_W_U,
        model=model,
        log_y=False,
        histnorm=None,
    )

# let's make a pretty color scheme
from plotly.colors import n_colors

colors = n_colors("rgb(5, 200, 200)", "rgb(200, 10, 10)", 13, colortype="rgb")

# Make a box plot of the skewness by layer
fig = px.box(
    W_U_stats_df_dec_all_layers,
    x="layer",
    y="skewness",
    color="layer",
    color_discrete_sequence=colors,
    height=600,
    width=1200,
    title="Skewness cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs",
    labels={"layer": "Layer", "skewnss": "Skewness"},
)
fig.update_xaxes(showticklabels=True, dtick=1)

# increase font size
fig.update_layout(font=dict(size=16))
fig.show()

# Make a box plot of the skewness by layer
fig = px.box(
    W_U_stats_df_dec_all_layers,
    x="layer",
    y="kurtosis",
    color="layer",
    color_discrete_sequence=colors,
    height=600,
    width=1200,
    log_y=True,
    title="log kurtosis cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs",
    labels={"layer": "Layer", "kurtosis": "Log Kurtosis"},
)
fig.update_xaxes(showticklabels=True, dtick=1)

# increase font size
fig.update_layout(font=dict(size=16))
fig.show()

# Commented out IPython magic to ensure Python compatibility.
try:
    import google.colab # type: ignore
    from google.colab import output
    COLAB = True
#     %pip install sae-vis==0.2.14
except:
    COLAB = False
    from IPython import get_ipython # type: ignore
    ipython = get_ipython(); assert ipython is not None
    ipython.run_line_magic("load_ext", "autoreload")
    ipython.run_line_magic("autoreload", "2")

# Standard imports
import torch
from datasets import load_dataset
import webbrowser
import os
from transformer_lens import utils, HookedTransformer
from datasets.arrow_dataset import Dataset
from huggingface_hub import hf_hub_download
import time

# Library imports
from sae_vis.utils_fns import get_device
from sae_vis.model_fns import AutoEncoder
from sae_vis.data_storing_fns import SaeVisData
from sae_vis.data_config_classes import SaeVisConfig
# from sae_lens.training.sparse_autoencoder import SparseAutoencoder

# Imports for displaying vis in Colab / notebook
import webbrowser
import http.server
import socketserver
import threading
PORT = 8000

device = get_device()
torch.set_grad_enabled(False);

config = SaeVisConfig(
    hook_point=sae.cfg.hook_name,
    features=list(range(256)),
    minibatch_size_features=64,
    minibatch_size_tokens=256,
    device="cuda",
    dtype="bfloat16"
)

SEQ_LEN = 128

# Load in the data (it's a Dataset object)
data = load_dataset("NeelNanda/code-10k", split="train")
assert isinstance(data, Dataset)

# Tokenize the data (using a utils function) and shuffle it
tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=SEQ_LEN) # type: ignore
tokenized_data = tokenized_data.shuffle(42)

# Get the tokens as a tensor
all_tokens = tokenized_data["tokens"]
assert isinstance(all_tokens, torch.Tensor)

print(all_tokens.shape)

# Generate data
data = SaeVisRunner(config).run(encoder=sae, model=model, tokens=all_tokens[:2048])

# Save feature-centric visualization
from sae_dashboard.data_writing_fns import save_feature_centric_vis
save_feature_centric_vis(sae_vis_data=data, filename="feature_dashboard.html")

save_prompt_centric_vis(
    sae_vis_data=data,
    prompt="This is a test prompt.",
    filename="prompt_dashboard.html"
)

def display_vis_inline(filename: str, height: int = 850):
    '''
    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each
    vis has a unique port without having to define a port within the function.
    '''
    if not(COLAB):
        webbrowser.open(filename);

    else:
        global PORT

        def serve(directory):
            os.chdir(directory)

            # Create a handler for serving files
            handler = http.server.SimpleHTTPRequestHandler

            # Create a socket server with the handler
            with socketserver.TCPServer(("", PORT), handler) as httpd:
                print(f"Serving files from {directory} on port {PORT}")
                httpd.serve_forever()

        thread = threading.Thread(target=serve, args=("/content",))
        thread.start()

        output.serve_kernel_port_as_iframe(PORT, path=f"/{filename}", height=height, cache_in_notebook=True)

        PORT += 1

display_vis_inline("feature_dashboard.html")

import pandas as pd

# Let's start by getting the top 10 logits for each feature
projection_onto_unembed = sparse_autoencoder.W_dec @ model.W_U


# get the top 10 logits.
vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)

# get 10 random features
random_indices = torch.randint(0, projection_onto_unembed.shape[0], (10,))

# Show the top 10 logits promoted by those features
top_10_logits_df = pd.DataFrame(
    [model.to_str_tokens(i) for i in inds[random_indices]],
    index=random_indices.tolist(),
).T
top_10_logits_df

gpt2_small_sparse_autoencoders

gpt2_small_sae_sparsities = {}

# let's make a pretty color scheme
from plotly.colors import n_colors

colors = n_colors("rgb(5, 200, 200)", "rgb(200, 10, 10)", 13, colortype="rgb")

# Make a box plot of the skewness by layer
fig = px.box(
    W_U_stats_df_dec_all_layers,
    x="layer",
    y="skewness",
    color="layer",
    color_discrete_sequence=colors,
    height=600,
    width=1200,
    title="Skewness cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs",
    labels={"layer": "Layer", "skewnss": "Skewness"},
)
fig.update_xaxes(showticklabels=True, dtick=1)

# increase font size
fig.update_layout(font=dict(size=16))
fig.show()

# Make a box plot of the skewness by layer
fig = px.box(
    W_U_stats_df_dec_all_layers,
    x="layer",
    y="kurtosis",
    color="layer",
    color_discrete_sequence=colors,
    height=600,
    width=1200,
    log_y=True,
    title="log kurtosis cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs",
    labels={"layer": "Layer", "kurtosis": "Log Kurtosis"},
)
fig.update_xaxes(showticklabels=True, dtick=1)

# increase font size
fig.update_layout(font=dict(size=16))
fig.show()





!pip install sae-lens

!pip install sae_dashboard

from sae_lens import SAE
from transformer_lens import HookedTransformer
from sae_dashboard.sae_vis_data import SaeVisConfig
from sae_dashboard.sae_vis_runner import SaeVisRunner

import numpy as np
import torch
import plotly_express as px

from transformer_lens import HookedTransformer

# Model Loading
from sae_lens import SAE
from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list

# Virtual Weight / Feature Statistics Functions
from sae_lens.analysis.feature_statistics import (
    get_all_stats_dfs,
    get_W_U_W_dec_stats_df,
)

# Enrichment Analysis Functions
from sae_lens.analysis.tsea import (
    get_enrichment_df,
    manhattan_plot_enrichment_scores,
    plot_top_k_feature_projections_by_token_and_category,
)
from sae_lens.analysis.tsea import (
    get_baby_name_sets,
    get_letter_gene_sets,
    generate_pos_sets,
    get_test_gene_sets,
    get_gene_set_from_regex,
)

# Commented out IPython magic to ensure Python compatibility.
try:
    import google.colab # type: ignore
    from google.colab import output
    COLAB = True
#     %pip install sae-vis==0.2.14
except:
    COLAB = False
    from IPython import get_ipython # type: ignore
    ipython = get_ipython(); assert ipython is not None
    ipython.run_line_magic("load_ext", "autoreload")
    ipython.run_line_magic("autoreload", "2")

# Standard imports
import torch
from datasets import load_dataset
import webbrowser
import os
from transformer_lens import utils, HookedTransformer
from datasets.arrow_dataset import Dataset
from huggingface_hub import hf_hub_download
import time

# Library imports
from sae_vis.utils_fns import get_device
from sae_vis.model_fns import AutoEncoder
from sae_vis.data_storing_fns import SaeVisData
from sae_vis.data_config_classes import SaeVisConfig
# from sae_lens.training.sparse_autoencoder import SparseAutoencoder

# Imports for displaying vis in Colab / notebook
import webbrowser
import http.server
import socketserver
import threading
PORT = 8000

device = get_device()
torch.set_grad_enabled(False);

load_path = "pretrained_code10k"
load_path_2 = "pretrained_tinycodes"
device = "cuda"  # or "cuda" if you want to load it on GPU
dtype = "float32"  # Optional: Specify the dtype if needed

# Load the Sparse Autoencoder
code10k_sae = SAE.load_from_pretrained(load_path, device=device, dtype=dtype)
tinycodes_sae = SAE.load_from_pretrained(load_path, device=device, dtype=dtype)

model = HookedTransformer.from_pretrained("gpt2-small", device="cuda", dtype="bfloat16")

pip install datasets

from datasets import load_dataset

SEQ_LEN = 128

# Load in the data (it's a Dataset object)
data = load_dataset("NeelNanda/code-10k", split="train")
# assert isinstance(data, Dataset)

# Tokenize the data (using a utils function) and shuffle it
tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=SEQ_LEN) # type: ignore
tokenized_data = tokenized_data.shuffle(42)

# Get the tokens as a tensor
all_tokens = tokenized_data["tokens"]
assert isinstance(all_tokens, torch.Tensor)

print(all_tokens.shape)

from sae_lens import SAE
from transformer_lens import HookedTransformer
from sae_dashboard.sae_vis_data import SaeVisConfig
from sae_dashboard.sae_vis_runner import SaeVisRunner

config = SaeVisConfig(
    hook_point=tinycodes_sae.cfg.hook_name,
    features=list(range(256)),
    device="cuda",
    dtype="bfloat16"
)

config.device="cuda",
config.dtype="bfloat16"

data = SaeVisRunner(config).run(encoder=code10k_sae, model=model, tokens=all_tokens)

from sae_dashboard.data_writing_fns import save_feature_centric_vis, save_prompt_centric_vis

save_feature_centric_vis(sae_vis_data=data, filename="feature_dashboard.html")

import logging
logging.basicConfig(level=logging.DEBUG)

import sys
sys.stdout = sys.__stdout__  # Reset stdout to ensure it isn't being captured

save_prompt_centric_vis(sae_vis_data=data, prompt="hello", filename="feature_dashboard.html")

def display_vis_inline(filename: str, height: int = 850):
    '''
    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each
    vis has a unique port without having to define a port within the function.
    '''
    if not(COLAB):
        webbrowser.open(filename);

    else:
        global PORT

        def serve(directory):
            os.chdir(directory)

            # Create a handler for serving files
            handler = http.server.SimpleHTTPRequestHandler

            # Create a socket server with the handler
            with socketserver.TCPServer(("", PORT), handler) as httpd:
                print(f"Serving files from {directory} on port {PORT}")
                httpd.serve_forever()

        thread = threading.Thread(target=serve, args=("/content",))
        thread.start()

        output.serve_kernel_port_as_iframe(PORT, path=f"/{filename}", height=height, cache_in_notebook=True)

        PORT += 1